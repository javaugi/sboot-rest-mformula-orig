/*
 * Click nbfs://nbhost/SystemFileSystem/Templates/Licenses/license-default.txt to change this license
 * Click nbfs://nbhost/SystemFileSystem/Templates/Classes/Class.java to edit this template
 */
package com.spring5.mongodb;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

@Service
public class MySparkService {
    
    //@Autowired
    private SparkSession sparkSession;

    public void processData(String inputPath, String outputPath) {
        // Load data using SparkSession
        Dataset<Row> data = sparkSession.read().format("csv").load(inputPath);

        // Perform transformations (example)
        data.show();

        // Write results
        data.write().format("csv").save(outputPath);
    }
}
/*
Apache Spark is a fast, general-purpose, distributed processing framework designed for large-scale data analytics and machine learning. It enables parallel processing of data across a cluster of computers, offering high speed and scalability compared to traditional methods. Spark is used for various big data tasks, including batch processing, stream processing, machine learning, and graph processing. 
Here's a more detailed explanation:
Key Features of Spark: 
Speed and Scalability:
.
Spark's in-memory processing and distributed architecture allow it to handle large datasets much faster than traditional disk-based systems. 
Unified Analytics Engine:
.
Spark provides a unified platform for various data processing tasks, including SQL, streaming, and machine learning, as well as graph processing. 
Fault Tolerance:
.
Spark automatically replicates data and processes across the cluster, ensuring that if a node fails, the workload can continue without interruption. 
Multi-Language APIs:
.
Spark offers APIs in Java, Scala, Python, and R, making it accessible to a wide range of data scientists and engineers. 
Rich Libraries:
.
Spark includes a suite of libraries for different tasks, such as Spark SQL for structured data processing, Spark MLlib for machine learning, and GraphX for graph processing. 
Use Cases of Spark:
ETL (Extract, Transform, Load): Processing large datasets from various sources and transforming them into a usable format. 
Batch Processing: Analyzing large datasets stored in files or databases. 
Real-time Analytics: Processing data streams from sensors, IoT devices, or financial systems. 
Machine Learning: Training and deploying machine learning models on large datasets. 
Graph Processing: Analyzing relationships between data points in graphs. 
In Summary:
Apache Spark is a powerful and versatile framework for handling big data tasks. Its speed, scalability, and flexibility make it a popular choice for data scientists, engineers, and organizations of all sizes. 
*/
